import requests
from bs4 import BeautifulSoup
import time

def scrape_event_list_pages():
    base_url = "https://visitseattle.org/events/page/"
    event_urls = []

    for page in range(1, 42):  # Assuming there are 41 pages to scrape
        url = f"{base_url}{page}"
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # Assuming the selector to find event detail links remains the same
            links = soup.select('div.search-result-preview > div > h3 > a')
            for link in links:
                event_urls.append(link['href'])
        else:
            print(f"Failed to retrieve page {page}")
        
        # Respect the website's server by sleeping for a second between requests
        time.sleep(1)

    return event_urls

# Scrape the event list pages and collect URLs
event_urls = scrape_event_list_pages()

# For demonstration, we'll print the number of URLs collected
print(f"Collected {len(event_urls)} event URLs.")

