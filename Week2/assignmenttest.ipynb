{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 369 event URLs.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_event_list_pages():\n",
    "    base_url = \"https://visitseattle.org/events/page/\"\n",
    "    event_urls = []\n",
    "\n",
    "    for page in range(1, 42):  # Assuming there are 41 pages to scrape\n",
    "        url = f\"{base_url}{page}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Assuming the selector to find event detail links remains the same\n",
    "            links = soup.select('div.search-result-preview > div > h3 > a')\n",
    "            for link in links:\n",
    "                event_urls.append(link['href'])\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page {page}\")\n",
    "        \n",
    "        # Respect the website's server by sleeping for a second between requests\n",
    "        time.sleep(1)\n",
    "\n",
    "    return event_urls\n",
    "\n",
    "# Scrape the event list pages and collect URLs\n",
    "event_urls = scrape_event_list_pages()\n",
    "\n",
    "# For demonstration, we'll print the number of URLs collected\n",
    "print(f\"Collected {len(event_urls)} event URLs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_event_details(url):\n",
    "    print(f\"Scraping event details from {url}\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Update these selectors based on the actual HTML structure\n",
    "        #name = soup.select_one('EVENT_NAME_SELECTOR').text.strip()\n",
    "        date = soup.select_one('EVENT_DATE_SELECTOR').text.strip()\n",
    "        location = soup.select_one('EVENT_LOCATION_SELECTOR').text.strip()\n",
    "        event_type = 'EVENT_TYPE_PLACEHOLDER'  # Modify as needed\n",
    "        region = 'EVENT_REGION_PLACEHOLDER'  # Modify as needed\n",
    "        return {\n",
    "            #'Name': name,\n",
    "            'Date': date,\n",
    "            'Location': location,\n",
    "            'Type': event_type,\n",
    "            'Region': region\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to retrieve details for {url}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_events_to_csv(events, filename='events.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['Name', 'Date', 'Location', 'Type', 'Region'])\n",
    "        writer.writeheader()\n",
    "        for event in events:\n",
    "            if event:  # Ensure event is not None\n",
    "                writer.writerow(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping event details from https://visitseattle.org/events/an-evening-with-lucia-micarelli/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted. Event details have been saved to event_test.csv.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 13\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m event_urls \u001b[38;5;241m=\u001b[39m scrape_event_list_pages()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Scrape details for each event\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m event_details \u001b[38;5;241m=\u001b[39m [scrape_event_details(url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m event_urls]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Save the event details to a CSV file, now named 'event_test.csv'\u001b[39;00m\n\u001b[0;32m      9\u001b[0m save_events_to_csv(event_details, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m event_urls \u001b[38;5;241m=\u001b[39m scrape_event_list_pages()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Scrape details for each event\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m event_details \u001b[38;5;241m=\u001b[39m [scrape_event_details(url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m event_urls]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Save the event details to a CSV file, now named 'event_test.csv'\u001b[39;00m\n\u001b[0;32m      9\u001b[0m save_events_to_csv(event_details, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36mscrape_event_details\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      5\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Update these selectors based on the actual HTML structure\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#name = soup.select_one('EVENT_NAME_SELECTOR').text.strip()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m date \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect_one(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEVENT_DATE_SELECTOR\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      9\u001b[0m location \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect_one(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEVENT_LOCATION_SELECTOR\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m event_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEVENT_TYPE_PLACEHOLDER\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Modify as needed\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Scrape the list of event URLs\n",
    "    event_urls = scrape_event_list_pages()\n",
    "\n",
    "    # Scrape details for each event\n",
    "    event_details = [scrape_event_details(url) for url in event_urls]\n",
    "\n",
    "    # Save the event details to a CSV file, now named 'event_test.csv'\n",
    "    save_events_to_csv(event_details, filename='event_test.csv')\n",
    "    print(\"Completed. Event details have been saved to event_test.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
